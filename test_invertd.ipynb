{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc8439f-fcb4-4aa3-8006-807981cc0b6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70486d3-23ac-4ca6-a642-561ffcdffb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning import (\n",
    "    LightningDataModule, \n",
    "    LightningModule, \n",
    "    Trainer, \n",
    ")\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from monai.data import CacheDataset, DataLoader, list_data_collate\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AddChanneld,\n",
    "    AsDiscreted,\n",
    "    BatchInverseTransform,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Rand3DElasticd,\n",
    "    RandAffined,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandShiftIntensityd,\n",
    "    RandZoomd,\n",
    "    SaveImaged, \n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.transforms import AsDiscrete\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Act, Norm \n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.config import print_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bc6e47-2fcb-440c-9168-95ecd8177696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.6.dev2123\n",
      "Numpy version: 1.19.2\n",
      "Pytorch version: 1.8.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 448c2527ad0c39aa7eb8a345ebd71e7f7985d374\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 8.2.0\n",
      "Tensorboard version: 2.4.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.9.1\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.61.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b0ff8-0042-401f-a353-ca42d7e15bb8",
   "metadata": {},
   "source": [
    "## Define DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1924bd48-c9d1-433f-819b-1e8dae1f62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeriDataModule(LightningDataModule):\n",
    "    \"\"\"\n",
    "    Example of LightningDataModule for MNIST dataset.\n",
    "\n",
    "    A DataModule implements 5 key methods:\n",
    "        - prepare_data (things to do on 1 GPU/TPU, not on every GPU/TPU in distributed mode)\n",
    "        - setup (things to do on every accelerator in distributed mode)\n",
    "        - train_dataloader (the training dataloader)\n",
    "        - val_dataloader (the validation dataloader(s))\n",
    "        - test_dataloader (the test dataloader(s))\n",
    "\n",
    "    This allows you to share a full dataset without explaining how to download,\n",
    "    split, transform and process the data\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"data/\",\n",
    "        train_batch_size: int = 2,\n",
    "        val_batch_size: int = 1,\n",
    "        num_workers: int = 4,\n",
    "        pin_memory: bool = False,\n",
    "        pix_dim: Sequence[float] = (1.5, 1.5, 2),\n",
    "        spatial_size: Union[Sequence[int],int] = (96, 96, 96),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.pix_dim = pix_dim\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "        # define the data transforms\n",
    "        self.train_transforms = Compose(\n",
    "                [\n",
    "                    # load the NIfTI files\n",
    "                    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                    # convert image to \"channel-first\" format\n",
    "                    AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                    # resample to a consistent voxel format\n",
    "                    Spacingd(\n",
    "                        keys=[\"image\", \"label\"],\n",
    "                        pixdim=self.pix_dim, \n",
    "                        mode=(\"bilinear\", \"nearest\"),\n",
    "                    ),\n",
    "                    # reorientate volumes to have a consistent axes orientation\n",
    "                    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                    \n",
    "                    ScaleIntensityRanged(\n",
    "                        keys=[\"image\"], a_min=-57, a_max=164,\n",
    "                        b_min=0.0, b_max=1.0, clip=True,\n",
    "                    ),\n",
    "                    \n",
    "                    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                    # randomly crop out patch samples from\n",
    "                    # big image based on pos / neg ratio\n",
    "                    # the image centers of negative samples\n",
    "                    # must be in valid image area\n",
    "                    RandCropByPosNegLabeld(\n",
    "                        keys=[\"image\", \"label\"],\n",
    "                        label_key=\"label\",\n",
    "                        spatial_size=self.spatial_size,\n",
    "                        pos=1,\n",
    "                        neg=1,\n",
    "                        num_samples=4,\n",
    "                        image_key=\"image\",\n",
    "                        image_threshold=0,\n",
    "                    ),\n",
    "                    ToTensord(keys=[\"image\", \"label\"]),\n",
    "                ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                AddChanneld(keys=[\"image\", \"label\"]),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=self.pix_dim,\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"], a_min=-57, a_max=164,\n",
    "                    b_min=0.0, b_max=1.0, clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                ToTensord(keys=[\"image\", \"label\"]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \n",
    "        # Load data -> Assign train/val datasets for use in dataloaders\n",
    "        \n",
    "        # set up the correct data path\n",
    "        train_images = sorted(\n",
    "            glob.glob(os.path.join(self.data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "        train_labels = sorted(\n",
    "            glob.glob(os.path.join(self.data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "        \n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name}\n",
    "            for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "        self.train_files, self.val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "        # we use cached datasets - these are 10x faster than regular datasets\n",
    "        self.val_ds = CacheDataset(\n",
    "                data=self.val_files, transform=self.val_transforms,\n",
    "                cache_rate=1.0, num_workers=4,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # apply transforms to train dataset in train_dataloader for data augmentation\n",
    "        # setup() is not run when we reload the train and val dataloaders\n",
    "        train_ds = CacheDataset(\n",
    "            data=self.train_files, transform=self.train_transforms,\n",
    "            cache_rate=1.0, num_workers=4,\n",
    "        )\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_ds, \n",
    "            batch_size=self.train_batch_size, \n",
    "            num_workers=self.num_workers, \n",
    "            collate_fn=list_data_collate,\n",
    "            pin_memory=self.pin_memory,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            dataset=self.val_ds, \n",
    "            batch_size=self.val_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory, \n",
    "            shuffle=False\n",
    "            )\n",
    "        return val_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pred_loader = torch.utils.data.DataLoader(\n",
    "            dataset=self.val_ds, \n",
    "            batch_size=self.val_batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory, \n",
    "            shuffle=False\n",
    "            )\n",
    "\n",
    "        self.post_transforms = Compose([\n",
    "            AsDiscreted(keys=\"pred\", argmax=True, to_onehot=False, n_classes=3),\n",
    "            Invertd(\n",
    "                keys=\"pred\",  # invert the `pred` data field, also support multiple fields\n",
    "                transform=self.val_transforms,\n",
    "                loader=pred_loader,\n",
    "                orig_keys=\"image\",  # get the previously applied pre_transforms information on the `img` data field,\n",
    "                                  # then invert `pred` based on this information. we can use same info\n",
    "                                  # for multiple fields, also support different orig_keys for different fields\n",
    "                meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\n",
    "                orig_meta_keys=\"image_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\n",
    "                                                 # for example, may need the `affine` to invert `Spacingd` transform,\n",
    "                                                 # multiple fields can use the same meta data to invert\n",
    "                meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\n",
    "                                               # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\n",
    "                                               # otherwise, no need this arg during inverting\n",
    "                nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\n",
    "                to_tensor=True,  # convert to PyTorch Tensor after inverting\n",
    "            ),\n",
    "            SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\n",
    "        ])\n",
    "        return pred_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbacb4-fa26-4a03-8a4a-0457c6e0978e",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56fb43f-9f64-4034-afa0-9acfa182d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeriLitModel(LightningModule):\n",
    "    \"\"\"\n",
    "    Example of LightningModule for MNIST classification.\n",
    "\n",
    "    A LightningModule organizes your PyTorch code into 5 sections:\n",
    "        - Computations (init).\n",
    "        - Train loop (training_step)\n",
    "        - Validation loop (validation_step)\n",
    "        - Test loop (test_step)\n",
    "        - Optimizers (configure_optimizers)\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_res_units: int = 2,\n",
    "        act=Act.PRELU,\n",
    "        norm=Norm.BATCH,\n",
    "        dropout: float = 0.0,\n",
    "        lr: float = 1e-4,\n",
    "        loss_function = \"dice_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line ensures params passed to LightningModule will be saved to ckpt\n",
    "        # it also allows to access params with 'self.hparams' attribute\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self._model = UNet(\n",
    "            dimensions=3,\n",
    "            in_channels=1,\n",
    "            out_channels=3, # 3 classes = background + trapezium + metacarpial\n",
    "            channels=(16, 32, 64, 128, 256),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            kernel_size=3,\n",
    "            up_kernel_size=3,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "            act=self.hparams.act,\n",
    "            norm=self.hparams.norm,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        \n",
    "        # define loss function\n",
    "        if self.hparams.loss_function == \"dice_loss\":\n",
    "            self.loss_function = DiceLoss(include_background=True,\n",
    "                to_onehot_y=True, softmax=True)\n",
    "        \n",
    "        elif self.hparams.loss_function == \"focal_loss_2\":\n",
    "            # give twice more weight to the trapezium class (class 1)\n",
    "            # than to the background or the metacarpus (classes 0 & 2)\n",
    "            self.loss_function = FocalLoss(include_background = True, \n",
    "                to_onehot_y = True, weight = [1.0, 2.0, 1.0])\n",
    "\n",
    "        else:\n",
    "            # give three times weight to the trapezium class (class 1)\n",
    "            # than to the background or the metacarpus (classes 0 & 2)\n",
    "            self.loss_function = FocalLoss(include_background = True, \n",
    "                to_onehot_y = True, weight = [1.0, 3.0, 1.0])\n",
    "\n",
    "        # execute after model forward to transform model output  to discrete values\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=True, n_classes=3)\n",
    "        self.post_label = AsDiscrete(to_onehot=True, n_classes=3)\n",
    "\n",
    "        self.dice_metric = DiceMetric(include_background=False, \n",
    "            reduction=\"mean\")\n",
    "\n",
    "\n",
    "    def forward(self, batch: Any):\n",
    "        images = batch[\"image\"]\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        logits = sliding_window_inference(\n",
    "            images, roi_size, sw_batch_size, self._model\n",
    "        )\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), self.hparams.lr)\n",
    "        return optimizer   \n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "        # in Lightning, we recommend to separate training from inference\n",
    "        # therefore, we don't call the forward method during training\n",
    "        logits = self._model(images)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self.forward(batch)\n",
    "        loss = self.loss_function(logits, labels)\n",
    "        self.log('val/loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # binarize predictions and convert them to onehot format for DiceMetric\n",
    "        outputs = self.post_pred(logits)\n",
    "        labels = self.post_label(labels)\n",
    "        value, not_nans = self.dice_metric(\n",
    "            y_pred=outputs,\n",
    "            y=labels,\n",
    "        )\n",
    "        not_nans = not_nans.item()\n",
    "        return {\"val_loss\": loss, \"val_dice\": value, \"not_nans\": not_nans}\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Any]):\n",
    "        val_dice, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            val_dice += output[\"val_dice\"].item() * output[\"not_nans\"]\n",
    "            num_items += output[\"not_nans\"]\n",
    "        mean_val_dice = torch.tensor(val_dice / num_items)\n",
    "        self.log('val/dice', mean_val_dice, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]=None):\n",
    "        batch[\"pred\"] = self.forward(batch)\n",
    "        self.trainer.datamodule.post_transforms(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c95ed-e308-4804-8fe2-fbb712df4039",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "You don't need to run the two following cells if you already have a checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d049a9b-b82e-4b4e-b0a6-aab0df4124d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize datamodule\n",
    "data_dir = \"/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen\"\n",
    "data = KeriDataModule(data_dir=data_dir)\n",
    "\n",
    "# initialize model\n",
    "model = KeriLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e1598-31d2-4050-9e53-41e959088371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/dice\",\n",
    "                                      save_top_k=1,\n",
    "                                      save_last=True,\n",
    "                                      mode=\"max\",\n",
    "                                      dirpath='/media/diane/Shared Data/Data/Spleen_Data/checkpoints/')\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(gpus=1, precision=32, amp_backend=\"native\",\n",
    "                  amp_level=\"02\", progress_bar_refresh_rate=20,\n",
    "                  max_epochs=300, callbacks=[checkpoint_callback])\n",
    "\n",
    "# train\n",
    "trainer.fit(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0f023-4adf-43fb-b6a7-0e0490d187f0",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f4960c-94a4-4a59-a8ff-3e049a7a6fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Loading dataset: 100%|██████████| 9/9 [00:06<00:00,  1.45it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]\n",
      "=== Transform input info -- method ===\n",
      "image statistics:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (1, 226, 157, 113)\n",
      "Value range: (0.0, 2.0)\n",
      "image_transforms statistics:\n",
      "Type: <class 'list'>\n",
      "Value: [{'class': 'Spacingd', 'id': 140359533164240, 'orig_size': [512, 512, 33], 'extra_info': {'meta_key': 'image_meta_dict', 'old_affine': tensor([[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    1.0000]], device='cuda:0',\n",
      "       dtype=torch.float64), 'mode': 'nearest', 'padding_mode': 'border', 'align_corners': 'none'}}, {'class': 'Orientationd', 'id': 140359533164336, 'orig_size': [239, 239, 113], 'extra_info': {'meta_key': 'image_meta_dict', 'old_affine': tensor([[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "        [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    1.0000]], device='cuda:0',\n",
      "       dtype=torch.float64)}}, {'class': 'CropForegroundd', 'id': 140359533164672, 'orig_size': [239, 239, 113], 'extra_info': {'box_start': tensor([ 3, 22,  0], device='cuda:0'), 'box_end': tensor([229, 179, 113], device='cuda:0')}}]\n",
      "image_meta_dict statistics:\n",
      "Type: <class 'dict'>\n",
      "Value: {'sizeof_hdr': 348, 'extents': 0, 'session_error': 0, 'dim_info': 0, 'dim': tensor([  3, 512, 512,  33,   1,   1,   1,   1], device='cuda:0',\n",
      "       dtype=torch.int16), 'intent_p1': 0.0, 'intent_p2': 0.0, 'intent_p3': 0.0, 'intent_code': 0, 'datatype': 16, 'bitpix': 32, 'slice_start': 0, 'pixdim': tensor([1.0000, 0.6992, 0.6992, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0'), 'vox_offset': 0.0, 'scl_slope': nan, 'scl_inter': nan, 'slice_end': 0, 'slice_code': 0, 'xyzt_units': 10, 'cal_max': 0.0, 'cal_min': 0.0, 'slice_duration': 0.0, 'toffset': 0.0, 'glmax': 0, 'glmin': 0, 'qform_code': 1, 'sform_code': 1, 'quatern_b': 0.0, 'quatern_c': 0.0, 'quatern_d': 0.0, 'qoffset_x': -357.3009033203125, 'qoffset_y': -357.3009033203125, 'qoffset_z': 0.0, 'srow_x': tensor([   0.6992,    0.0000,    0.0000, -357.3009], device='cuda:0'), 'srow_y': tensor([   0.0000,    0.6992,    0.0000, -357.3009], device='cuda:0'), 'srow_z': tensor([0.0000, 0.0000, 7.0000, 0.0000], device='cuda:0'), 'affine': tensor([[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "        [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    1.0000]], device='cuda:0',\n",
      "       dtype=torch.float64), 'original_affine': tensor([[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "        [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    1.0000]], device='cuda:0',\n",
      "       dtype=torch.float64), 'as_closest_canonical': False, 'spatial_shape': tensor([512, 512,  33], device='cuda:0', dtype=torch.int16), 'original_channel_dim': 'no_channel', 'filename_or_obj': '/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen/imagesTr/spleen_56.nii.gz'}\n",
      "\n",
      "=== Transform input info -- Invertd ===\n",
      "\n",
      "=== Transform input info -- Invertd ===\n",
      "image statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 1.0)\n",
      "image statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 1.0)\n",
      "label statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 1.0)\n",
      "label statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 1.0)\n",
      "image_meta_dict statistics:\n",
      "Type: <class 'dict'>\n",
      "Value: {'sizeof_hdr': tensor([348], device='cuda:0', dtype=torch.int32), 'extents': tensor([0], device='cuda:0', dtype=torch.int32), 'session_error': tensor([0], device='cuda:0', dtype=torch.int16), 'dim_info': tensor([0], device='cuda:0', dtype=torch.uint8), 'dim': tensor([[  3, 512, 512,  33,   1,   1,   1,   1]], device='cuda:0',\n",
      "       dtype=torch.int16), 'intent_p1': tensor([0.], device='cuda:0'), 'intent_p2': tensor([0.], device='cuda:0'), 'intent_p3': tensor([0.], device='cuda:0'), 'intent_code': tensor([0], device='cuda:0', dtype=torch.int16), 'datatype': tensor([16], device='cuda:0', dtype=torch.int16), 'bitpix': tensor([32], device='cuda:0', dtype=torch.int16), 'slice_start': tensor([0], device='cuda:0', dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.6992, 0.6992, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0'), 'vox_offset': tensor([0.], device='cuda:0'), 'scl_slope': tensor([nan], device='cuda:0'), 'scl_inter': tensor([nan], device='cuda:0'), 'slice_end': tensor([0], device='cuda:0', dtype=torch.int16), 'slice_code': tensor([0], device='cuda:0', dtype=torch.uint8), 'xyzt_units': tensor([10], device='cuda:0', dtype=torch.uint8), 'cal_max': tensor([0.], device='cuda:0'), 'cal_min': tensor([0.], device='cuda:0'), 'slice_duration': tensor([0.], device='cuda:0'), 'toffset': tensor([0.], device='cuda:0'), 'glmax': tensor([0], device='cuda:0', dtype=torch.int32), 'glmin': tensor([0], device='cuda:0', dtype=torch.int32), 'qform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'sform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'quatern_b': tensor([0.], device='cuda:0'), 'quatern_c': tensor([0.], device='cuda:0'), 'quatern_d': tensor([0.], device='cuda:0'), 'qoffset_x': tensor([-357.3009], device='cuda:0'), 'qoffset_y': tensor([-357.3009], device='cuda:0'), 'qoffset_z': tensor([0.], device='cuda:0'), 'srow_x': tensor([[   0.6992,    0.0000,    0.0000, -357.3009]], device='cuda:0'), 'srow_y': tensor([[   0.0000,    0.6992,    0.0000, -357.3009]], device='cuda:0'), 'srow_z': tensor([[0.0000, 0.0000, 7.0000, 0.0000]], device='cuda:0'), 'affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'original_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'as_closest_canonical': tensor([False], device='cuda:0'), 'spatial_shape': tensor([[512, 512,  33]], device='cuda:0', dtype=torch.int16), 'original_channel_dim': ['no_channel'], 'filename_or_obj': ['/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen/imagesTr/spleen_56.nii.gz']}\n",
      "image_meta_dict statistics:\n",
      "Type: <class 'dict'>\n",
      "Value: {'sizeof_hdr': tensor([348], device='cuda:0', dtype=torch.int32), 'extents': tensor([0], device='cuda:0', dtype=torch.int32), 'session_error': tensor([0], device='cuda:0', dtype=torch.int16), 'dim_info': tensor([0], device='cuda:0', dtype=torch.uint8), 'dim': tensor([[  3, 512, 512,  33,   1,   1,   1,   1]], device='cuda:0',\n",
      "       dtype=torch.int16), 'intent_p1': tensor([0.], device='cuda:0'), 'intent_p2': tensor([0.], device='cuda:0'), 'intent_p3': tensor([0.], device='cuda:0'), 'intent_code': tensor([0], device='cuda:0', dtype=torch.int16), 'datatype': tensor([16], device='cuda:0', dtype=torch.int16), 'bitpix': tensor([32], device='cuda:0', dtype=torch.int16), 'slice_start': tensor([0], device='cuda:0', dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.6992, 0.6992, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0'), 'vox_offset': tensor([0.], device='cuda:0'), 'scl_slope': tensor([nan], device='cuda:0'), 'scl_inter': tensor([nan], device='cuda:0'), 'slice_end': tensor([0], device='cuda:0', dtype=torch.int16), 'slice_code': tensor([0], device='cuda:0', dtype=torch.uint8), 'xyzt_units': tensor([10], device='cuda:0', dtype=torch.uint8), 'cal_max': tensor([0.], device='cuda:0'), 'cal_min': tensor([0.], device='cuda:0'), 'slice_duration': tensor([0.], device='cuda:0'), 'toffset': tensor([0.], device='cuda:0'), 'glmax': tensor([0], device='cuda:0', dtype=torch.int32), 'glmin': tensor([0], device='cuda:0', dtype=torch.int32), 'qform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'sform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'quatern_b': tensor([0.], device='cuda:0'), 'quatern_c': tensor([0.], device='cuda:0'), 'quatern_d': tensor([0.], device='cuda:0'), 'qoffset_x': tensor([-357.3009], device='cuda:0'), 'qoffset_y': tensor([-357.3009], device='cuda:0'), 'qoffset_z': tensor([0.], device='cuda:0'), 'srow_x': tensor([[   0.6992,    0.0000,    0.0000, -357.3009]], device='cuda:0'), 'srow_y': tensor([[   0.0000,    0.6992,    0.0000, -357.3009]], device='cuda:0'), 'srow_z': tensor([[0.0000, 0.0000, 7.0000, 0.0000]], device='cuda:0'), 'affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'original_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'as_closest_canonical': tensor([False], device='cuda:0'), 'spatial_shape': tensor([[512, 512,  33]], device='cuda:0', dtype=torch.int16), 'original_channel_dim': ['no_channel'], 'filename_or_obj': ['/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen/imagesTr/spleen_56.nii.gz']}\n",
      "label_meta_dict statistics:\n",
      "Type: <class 'dict'>\n",
      "Value: {'sizeof_hdr': tensor([348], device='cuda:0', dtype=torch.int32), 'extents': tensor([0], device='cuda:0', dtype=torch.int32), 'session_error': tensor([0], device='cuda:0', dtype=torch.int16), 'dim_info': tensor([0], device='cuda:0', dtype=torch.uint8), 'dim': tensor([[  3, 512, 512,  33,   1,   1,   1,   1]], device='cuda:0',\n",
      "       dtype=torch.int16), 'intent_p1': tensor([0.], device='cuda:0'), 'intent_p2': tensor([0.], device='cuda:0'), 'intent_p3': tensor([0.], device='cuda:0'), 'intent_code': tensor([0], device='cuda:0', dtype=torch.int16), 'datatype': tensor([2], device='cuda:0', dtype=torch.int16), 'bitpix': tensor([8], device='cuda:0', dtype=torch.int16), 'slice_start': tensor([0], device='cuda:0', dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.6992, 0.6992, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0'), 'vox_offset': tensor([0.], device='cuda:0'), 'scl_slope': tensor([nan], device='cuda:0'), 'scl_inter': tensor([nan], device='cuda:0'), 'slice_end': tensor([0], device='cuda:0', dtype=torch.int16), 'slice_code': tensor([0], device='cuda:0', dtype=torch.uint8), 'xyzt_units': tensor([10], device='cuda:0', dtype=torch.uint8), 'cal_max': tensor([0.], device='cuda:0'), 'cal_min': tensor([0.], device='cuda:0'), 'slice_duration': tensor([0.], device='cuda:0'), 'toffset': tensor([0.], device='cuda:0'), 'glmax': tensor([0], device='cuda:0', dtype=torch.int32), 'glmin': tensor([0], device='cuda:0', dtype=torch.int32), 'qform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'sform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'quatern_b': tensor([0.], device='cuda:0'), 'quatern_c': tensor([0.], device='cuda:0'), 'quatern_d': tensor([0.], device='cuda:0'), 'qoffset_x': tensor([-357.3009], device='cuda:0'), 'qoffset_y': tensor([-357.3009], device='cuda:0'), 'qoffset_z': tensor([0.], device='cuda:0'), 'srow_x': tensor([[   0.6992,    0.0000,    0.0000, -357.3009]], device='cuda:0'), 'srow_y': tensor([[   0.0000,    0.6992,    0.0000, -357.3009]], device='cuda:0'), 'srow_z': tensor([[0.0000, 0.0000, 7.0000, 0.0000]], device='cuda:0'), 'affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'original_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'as_closest_canonical': tensor([False], device='cuda:0'), 'spatial_shape': tensor([[512, 512,  33]], device='cuda:0', dtype=torch.int16), 'original_channel_dim': ['no_channel'], 'filename_or_obj': ['/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen/labelsTr/spleen_56.nii.gz']}\n",
      "label_meta_dict statistics:\n",
      "Type: <class 'dict'>\n",
      "Value: {'sizeof_hdr': tensor([348], device='cuda:0', dtype=torch.int32), 'extents': tensor([0], device='cuda:0', dtype=torch.int32), 'session_error': tensor([0], device='cuda:0', dtype=torch.int16), 'dim_info': tensor([0], device='cuda:0', dtype=torch.uint8), 'dim': tensor([[  3, 512, 512,  33,   1,   1,   1,   1]], device='cuda:0',\n",
      "       dtype=torch.int16), 'intent_p1': tensor([0.], device='cuda:0'), 'intent_p2': tensor([0.], device='cuda:0'), 'intent_p3': tensor([0.], device='cuda:0'), 'intent_code': tensor([0], device='cuda:0', dtype=torch.int16), 'datatype': tensor([2], device='cuda:0', dtype=torch.int16), 'bitpix': tensor([8], device='cuda:0', dtype=torch.int16), 'slice_start': tensor([0], device='cuda:0', dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.6992, 0.6992, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0'), 'vox_offset': tensor([0.], device='cuda:0'), 'scl_slope': tensor([nan], device='cuda:0'), 'scl_inter': tensor([nan], device='cuda:0'), 'slice_end': tensor([0], device='cuda:0', dtype=torch.int16), 'slice_code': tensor([0], device='cuda:0', dtype=torch.uint8), 'xyzt_units': tensor([10], device='cuda:0', dtype=torch.uint8), 'cal_max': tensor([0.], device='cuda:0'), 'cal_min': tensor([0.], device='cuda:0'), 'slice_duration': tensor([0.], device='cuda:0'), 'toffset': tensor([0.], device='cuda:0'), 'glmax': tensor([0], device='cuda:0', dtype=torch.int32), 'glmin': tensor([0], device='cuda:0', dtype=torch.int32), 'qform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'sform_code': tensor([1], device='cuda:0', dtype=torch.int16), 'quatern_b': tensor([0.], device='cuda:0'), 'quatern_c': tensor([0.], device='cuda:0'), 'quatern_d': tensor([0.], device='cuda:0'), 'qoffset_x': tensor([-357.3009], device='cuda:0'), 'qoffset_y': tensor([-357.3009], device='cuda:0'), 'qoffset_z': tensor([0.], device='cuda:0'), 'srow_x': tensor([[   0.6992,    0.0000,    0.0000, -357.3009]], device='cuda:0'), 'srow_y': tensor([[   0.0000,    0.6992,    0.0000, -357.3009]], device='cuda:0'), 'srow_z': tensor([[0.0000, 0.0000, 7.0000, 0.0000]], device='cuda:0'), 'affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'original_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'as_closest_canonical': tensor([False], device='cuda:0'), 'spatial_shape': tensor([[512, 512,  33]], device='cuda:0', dtype=torch.int16), 'original_channel_dim': ['no_channel'], 'filename_or_obj': ['/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen/labelsTr/spleen_56.nii.gz']}\n",
      "image_transforms statistics:\n",
      "Type: <class 'list'>\n",
      "Value: [{'class': ['Spacingd'], 'id': tensor([140359533164240], device='cuda:0'), 'orig_size': [tensor([512], device='cuda:0'), tensor([512], device='cuda:0'), tensor([33], device='cuda:0')], 'extra_info': {'meta_key': ['image_meta_dict'], 'old_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'mode': ['bilinear'], 'padding_mode': ['border'], 'align_corners': tensor([False], device='cuda:0')}}, {'class': ['Orientationd'], 'id': tensor([140359533164336], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'meta_key': ['image_meta_dict'], 'old_affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64)}}, {'class': ['CropForegroundd'], 'id': tensor([140359533164672], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'box_start': tensor([[ 3, 22,  0]], device='cuda:0'), 'box_end': tensor([[229, 179, 113]], device='cuda:0')}}, {'class': ['ToTensord'], 'id': tensor([140359533164816], device='cuda:0'), 'orig_size': [tensor([226], device='cuda:0'), tensor([157], device='cuda:0'), tensor([113], device='cuda:0')]}]\n",
      "image_transforms statistics:\n",
      "Type: <class 'list'>\n",
      "Value: [{'class': ['Spacingd'], 'id': tensor([140359533164240], device='cuda:0'), 'orig_size': [tensor([512], device='cuda:0'), tensor([512], device='cuda:0'), tensor([33], device='cuda:0')], 'extra_info': {'meta_key': ['image_meta_dict'], 'old_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'mode': ['bilinear'], 'padding_mode': ['border'], 'align_corners': tensor([False], device='cuda:0')}}, {'class': ['Orientationd'], 'id': tensor([140359533164336], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'meta_key': ['image_meta_dict'], 'old_affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64)}}, {'class': ['CropForegroundd'], 'id': tensor([140359533164672], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'box_start': tensor([[ 3, 22,  0]], device='cuda:0'), 'box_end': tensor([[229, 179, 113]], device='cuda:0')}}, {'class': ['ToTensord'], 'id': tensor([140359533164816], device='cuda:0'), 'orig_size': [tensor([226], device='cuda:0'), tensor([157], device='cuda:0'), tensor([113], device='cuda:0')]}]\n",
      "label_transforms statistics:\n",
      "Type: <class 'list'>\n",
      "Value: [{'class': ['Spacingd'], 'id': tensor([140359533164240], device='cuda:0'), 'orig_size': [tensor([512], device='cuda:0'), tensor([512], device='cuda:0'), tensor([33], device='cuda:0')], 'extra_info': {'meta_key': ['label_meta_dict'], 'old_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'mode': ['nearest'], 'padding_mode': ['border'], 'align_corners': tensor([False], device='cuda:0')}}, {'class': ['Orientationd'], 'id': tensor([140359533164336], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'meta_key': ['label_meta_dict'], 'old_affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64)}}, {'class': ['CropForegroundd'], 'id': tensor([140359533164672], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'box_start': tensor([[ 3, 22,  0]], device='cuda:0'), 'box_end': tensor([[229, 179, 113]], device='cuda:0')}}, {'class': ['ToTensord'], 'id': tensor([140359533164816], device='cuda:0'), 'orig_size': [tensor([226], device='cuda:0'), tensor([157], device='cuda:0'), tensor([113], device='cuda:0')]}]\n",
      "label_transforms statistics:\n",
      "Type: <class 'list'>\n",
      "Value: [{'class': ['Spacingd'], 'id': tensor([140359533164240], device='cuda:0'), 'orig_size': [tensor([512], device='cuda:0'), tensor([512], device='cuda:0'), tensor([33], device='cuda:0')], 'extra_info': {'meta_key': ['label_meta_dict'], 'old_affine': tensor([[[   0.6992,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.6992,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    7.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64), 'mode': ['nearest'], 'padding_mode': ['border'], 'align_corners': tensor([False], device='cuda:0')}}, {'class': ['Orientationd'], 'id': tensor([140359533164336], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'meta_key': ['label_meta_dict'], 'old_affine': tensor([[[   1.5000,    0.0000,    0.0000, -357.3009],\n",
      "         [   0.0000,    1.5000,    0.0000, -357.3009],\n",
      "         [   0.0000,    0.0000,    2.0000,    0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,    1.0000]]], device='cuda:0',\n",
      "       dtype=torch.float64)}}, {'class': ['CropForegroundd'], 'id': tensor([140359533164672], device='cuda:0'), 'orig_size': [tensor([239], device='cuda:0'), tensor([239], device='cuda:0'), tensor([113], device='cuda:0')], 'extra_info': {'box_start': tensor([[ 3, 22,  0]], device='cuda:0'), 'box_end': tensor([[229, 179, 113]], device='cuda:0')}}, {'class': ['ToTensord'], 'id': tensor([140359533164816], device='cuda:0'), 'orig_size': [tensor([226], device='cuda:0'), tensor([157], device='cuda:0'), tensor([113], device='cuda:0')]}]\n",
      "foreground_start_coord statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 3])\n",
      "Value range: (0, 22)\n",
      "foreground_start_coord statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 3])\n",
      "Value range: (0, 22)\n",
      "foreground_end_coord statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 3])\n",
      "Value range: (113, 229)\n",
      "foreground_end_coord statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 3])\n",
      "Value range: (113, 229)\n",
      "pred statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 2.0)\n",
      "pred statistics:\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([1, 1, 226, 157, 113])\n",
      "Value range: (0.0, 2.0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.post.dictionary.Invertd object at 0x7fa7fc7a8b20>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/croppad/dictionary.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mextra_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInverseKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXTRA_INFO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mbox_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"box_start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0mbox_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"box_end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/inverse_batch_transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/utils/misc.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(iterable, default)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/inverse_batch_transform.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvertible_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvertible_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0m_log_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"applying transform {transform}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <bound method CropForegroundd.inverse of <monai.transforms.croppad.dictionary.CropForegroundd object at 0x7fa8001d7880>>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/post/dictionary.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mallow_missing_keys_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0minverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/inverse_batch_transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mre_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\nMONAI hint: try creating `BatchInverseTransform` with `collate_fn=lambda x: x`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <bound method CropForegroundd.inverse of <monai.transforms.croppad.dictionary.CropForegroundd object at 0x7fa8001d7880>>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1df43086b79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_predicting\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_predicting\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# double dispatch to initiate the predicting loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# call hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/trainer/predict_loop.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cb6fcf6f8425>\u001b[0m in \u001b[0;36mpredict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_transform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/invertd/lib/python3.8/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0m_log_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"applying transform {transform}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.post.dictionary.Invertd object at 0x7fa7fc7a8b20>"
     ]
    }
   ],
   "source": [
    "# load model from checkpoint\n",
    "CKPT_PATH = \"/home/diane/Documents/Invertd_demo/epoch=195-step=3135.ckpt\"\n",
    "trained_model = KeriLitModel.load_from_checkpoint(checkpoint_path=CKPT_PATH)\n",
    "\n",
    "# initialize datamodule\n",
    "data_dir = \"/media/diane/Shared Data/Data/Spleen_Data/Task09_Spleen\"\n",
    "data = KeriDataModule(data_dir=data_dir)\n",
    "\n",
    "# initialize trainer for predict\n",
    "trainer = Trainer(gpus=1, precision=32, amp_backend=\"native\",\n",
    "                  amp_level=\"02\", progress_bar_refresh_rate=20)\n",
    "\n",
    "# train\n",
    "trainer.predict(model=trained_model, datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
